{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cvxopt.solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Kernel:\n",
    "    \"\"\"Check kernels here https://en.wikipedia.org/wiki/Support_vector_machine\"\"\"\n",
    "    @staticmethod\n",
    "    def linear():\n",
    "        return lambda x, y: np.inner(x, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian(sigma):\n",
    "        gamma = 1/(2*sigma**2)\n",
    "        return lambda x, y: math.exp(-gamma*(np.linalg.norm(x-y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6065306597126334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = Kernel()\n",
    "k.gaussian(1)(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 matrix, tc='i'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3]])\n",
    "cvxopt.matrix(np.outer(x, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SUPPORT_VECTOR_MULTIPLIER = 1e-5\n",
    "\n",
    "\n",
    "class SVMTrainer:\n",
    "    def __init__(self, kernel, c=0.1):\n",
    "        self._kernel = kernel\n",
    "        self._c = c\n",
    "\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "            X: martix of features\n",
    "            y: vector of labels\n",
    "\n",
    "            next step: Compute lagrange multipliers by calling _compute_lagrange_multipliers method\n",
    "            retrun:    Return Predictor object by calling _create_predictor method\n",
    "        \"\"\"\n",
    "        lagrange_multipliers = self._compute_lagrange_multipliers(X, y)\n",
    "        return self._create_predictor(X, y, lagrange_multipliers)\n",
    "\n",
    "\n",
    "    def _kernel_matrix(self, X):\n",
    "        \"\"\"\n",
    "            X: martix of features\n",
    "\n",
    "            next step: Get number of samples\n",
    "            next step: Create zero matrix of quadratic shape of number of samples \n",
    "            next step: Calculate kernels\n",
    "            retrun:    Return Kernels matrix\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "\n",
    "        print(X)\n",
    "\n",
    "        for i, x_i in enumerate(X):\n",
    "            for j, x_j in enumerate(X):\n",
    "                K[i, j] = self._kernel(x_i, x_j)\n",
    "\n",
    "        return K\n",
    "\n",
    "\n",
    "    def _create_predictor(self, X, y, lagrange_multipliers):\n",
    "        \"\"\"\n",
    "            X: martix of features\n",
    "            y: vector of labels\n",
    "            lagrange_multipliers: vector of langrange multipliers\n",
    "\n",
    "            next step: Get non-zero lagrange multipliers indicies\n",
    "            next step: Get non-zero lagrange multipliers\n",
    "            next step: Get support vecorts\n",
    "            next step: Get support vecort labels\n",
    "            next step: Ð¡ompute bias (use avg trick)\n",
    "            retrun   : Return SVMPredictor object\n",
    "        \"\"\"\n",
    "\n",
    "        support_vector_indices = lagrange_multipliers > MIN_SUPPORT_VECTOR_MULTIPLIER\n",
    "\n",
    "        support_multipliers = lagrange_multipliers[support_vector_indices]\n",
    "\n",
    "        support_vectors = X[support_vector_indices]\n",
    "\n",
    "        support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        bias = np.mean(\n",
    "            [y_k - SVMPredictor(\n",
    "                    kernel=self._kernel,\n",
    "                    bias=0.0,\n",
    "                    weights=support_multipliers,\n",
    "                    support_vectors=support_vectors,\n",
    "                    support_vector_labels=support_vector_labels\n",
    "                ).predict(x_k) for (y_k, x_k) in zip(support_vector_labels, support_vectors)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return SVMPredictor(\n",
    "            kernel=self._kernel,\n",
    "            bias=0.0,\n",
    "            weights=support_multipliers,\n",
    "            support_vectors=support_vectors,\n",
    "            support_vector_labels=support_vector_labels\n",
    "        )\n",
    "\n",
    "\n",
    "    def _compute_lagrange_multipliers(self, X, y):\n",
    "        \"\"\"\n",
    "            X: martix of features\n",
    "            y: vector of labels\n",
    "\n",
    "\n",
    "            Need to Solve\n",
    "                min 1/2 x^T P x + q^T x (aplha is x)\n",
    "                s.t.\n",
    "                    Gx <= h (alpha >= 0)\n",
    "                    Ax = b (y^T * alpha = 0)\n",
    "\n",
    "\n",
    "            next step: Get number of samples\n",
    "            next step: Create Kernel matrix by calling _kernel_matrix method\n",
    "            next step: Create create quadratic term P based on Kernel matrix\n",
    "            next step: Create linear term q\n",
    "            next step: Create G, h, A, b\n",
    "            next step: Solve with - cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "            retrun:    Return flatten solution['x']\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        K = self._kernel_matrix(X)\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y, y) * K)\n",
    "\n",
    "        q = cvxopt.matrix(-1 * np.ones(n_samples))\n",
    "\n",
    "        G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "\n",
    "        h = cvxopt.matrix(np.zeros(n_samples))\n",
    "\n",
    "        A = cvxopt.matrix(y, (1, n_samples))\n",
    "\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "\n",
    "        # Check this\n",
    "\n",
    "        # -a_i \\leq 0\n",
    "        # G_std = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "        # h_std = cvxopt.matrix(np.zeros(n_samples))\n",
    "\n",
    "        # # a_i \\leq c\n",
    "        # G_slack = cvxopt.matrix(np.diag(np.ones(n_samples)))\n",
    "        # h_slack = cvxopt.matrix(np.ones(n_samples) * self._c)\n",
    "\n",
    "        # G = cvxopt.matrix(np.vstack((G_std, G_slack)))\n",
    "        # h = cvxopt.matrix(np.vstack((h_std, h_slack)))\n",
    "\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        return np.ravel(solution['x'])\n",
    "\n",
    "\n",
    "class SVMPredictor(object):\n",
    "    def __init__(\n",
    "                self,\n",
    "                kernel,\n",
    "                bias,\n",
    "                weights,\n",
    "                support_vectors,\n",
    "                support_vector_labels\n",
    "            ):\n",
    "        \n",
    "        self._kernel = kernel\n",
    "        self._bias = bias\n",
    "        self._weights = weights\n",
    "        self._support_vectors = support_vectors\n",
    "        self._support_vector_labels = support_vector_labels\n",
    "\n",
    "\n",
    "        assert len(support_vectors) == len(support_vector_labels)\n",
    "        assert len(weights) == len(support_vector_labels)\n",
    "\n",
    "\n",
    "        logging.info(\"Bias: %s\", self._bias)\n",
    "        logging.info(\"Weights: %s\", self._weights)\n",
    "        logging.info(\"Support vectors: %s\", self._support_vectors)\n",
    "        logging.info(\"Support vector labels: %s\", self._support_vector_labels)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Computes the SVM prediction on the given features x.\n",
    "        \"\"\"\n",
    "        result = self._bias\n",
    "        for w_i, x_i, y_i in zip(self._weights,\n",
    "                                 self._support_vectors,\n",
    "                                 self._support_vector_labels):\n",
    "            result += w_i * y_i * self._kernel(x_i, x)\n",
    "\n",
    "        return np.sign(result).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import svm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def example(num_samples=10, num_features=2, grid_size=500):\n",
    "    \n",
    "    samples = np.matrix(np.random.normal(size=num_samples * num_features)\n",
    "                        .reshape(num_samples, num_features))\n",
    "    \n",
    "    labels = 2 * (samples.sum(axis=1) > 0) - 1.0\n",
    "    \n",
    "    trainer = svm.SVMTrainer(svm.Kernel.linear())\n",
    "    print(trainer._kernel_matrix(samples).shape)\n",
    "    predictor = trainer.train(samples, labels)\n",
    "\n",
    "    plot(predictor, samples, labels, grid_size)\n",
    "\n",
    "\n",
    "def plot(predictor, X, y, grid_size):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, grid_size),\n",
    "        np.linspace(y_min, y_max, grid_size),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    flatten = lambda m: np.array(m).reshape(-1,)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for (i, j) in itertools.product(range(grid_size), range(grid_size)):\n",
    "        point = np.array([xx[i, j], yy[i, j]]).reshape(1, 2)\n",
    "        result.append(predictor.predict(point))\n",
    "\n",
    "    Z = np.array(result).reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(\n",
    "        xx, yy, Z,\n",
    "        cmap=cm.Paired,\n",
    "        levels=[-0.01, 0.01],\n",
    "        extend='both',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    \n",
    "    plt.scatter(\n",
    "        flatten(X[:, 0]),\n",
    "        flatten(X[:, 1]),\n",
    "        c=flatten(y),\n",
    "        cmap=cm.Paired\n",
    "    )\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15947784  1.12776845]\n",
      " [ 0.44961451 -0.53339564]\n",
      " [ 0.91100516 -1.00871078]\n",
      " [ 0.93014191 -0.11386352]\n",
      " [-2.52753139 -0.51489152]\n",
      " [-0.90650316  0.67771234]\n",
      " [ 0.6509031  -0.26659312]\n",
      " [-1.36467578  0.19076284]\n",
      " [-0.48556435 -0.59596027]\n",
      " [-0.37170428  1.03260993]]\n",
      "(10, 10)\n",
      "[[ 1.15947784  1.12776845]\n",
      " [ 0.44961451 -0.53339564]\n",
      " [ 0.91100516 -1.00871078]\n",
      " [ 0.93014191 -0.11386352]\n",
      " [-2.52753139 -0.51489152]\n",
      " [-0.90650316  0.67771234]\n",
      " [ 0.6509031  -0.26659312]\n",
      " [-1.36467578  0.19076284]\n",
      " [-0.48556435 -0.59596027]\n",
      " [-0.37170428  1.03260993]]\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6829e+00 -1.0254e+01  3e+01  5e+00  2e+00\n",
      " 1: -1.2637e+01 -1.2972e+01  7e+00  2e+00  6e-01\n",
      " 2: -1.8842e+01 -2.1596e+01  1e+01  9e-01  4e-01\n",
      " 3: -1.8309e+01 -1.8889e+01  2e+00  1e-01  4e-02\n",
      " 4: -1.8029e+01 -1.8039e+01  2e-02  1e-03  4e-04\n",
      " 5: -1.8028e+01 -1.8028e+01  2e-04  1e-05  4e-06\n",
      " 6: -1.8028e+01 -1.8028e+01  2e-06  1e-07  4e-08\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bca4bff8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
